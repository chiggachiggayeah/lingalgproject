Andrew_Campbell
CS
StudentLife:_Assessing_Mental_Health,
ABSTRACT
Much of the stress and strain of student life remains hidden.
The StudentLife continuous sensing app assesses the day-today
and week-by-week impact of workload on stress, sleep,
activity, mood, sociability, mental well-being and academic
performance of a single class of 48 students across a 10 week
term at Dartmouth College using Android phones. Results
from the StudentLife study show a number of significant correlations
between the automatic objective sensor data from
smartphones and mental health and educational outcomes of
the student body. We also identify a Dartmouth term lifecycle
in the data that shows students start the term with high positive
affect and conversation levels, low stress, and healthy
sleep and daily activity patterns. As the term progresses and
the workload increases, stress appreciably rises while positive
affect, sleep, conversation and activity drops off. The
StudentLife dataset is publicly available on the web.
Author Keywords
Smartphone sensing; data analysis; mental health; academic
performance; behavioral trends
ACM Classification Keywords
H.1.2 User/Machine Systems; I.5 Pattern Recognition; J.3
Life and Medical Sciences
General Terms
Algorithms, Experimentation.
INTRODUCTION
Many questions arise when we think about the academic performance
of college students. Why do some students do better
than others? Under similar conditions, why do some individuals
excel while others fail? Why do students burnout,
drop classes, even drop out of college? What is the impact of
stress, mood, workload, sociability, sleep and mental wellbeing
on educational performance? In this paper, we use
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from Permissions@acm.org.
Ubicomp ’14, September 13 - 17 2014, Seattle, WA, USA
Copyright 2014 ACM 978-1-4503-2968-2/14/09...$15.00.
http://dx.doi.org/10.1145/2632048.2632054
smartphones carried by students to find answers to some of
these pressing questions.
Consider students at Dartmouth College, an Ivy League college
in a small New England college town. Students typically
take three classes over a 10-week term and live on campus.
Dartmouth classes are generally demanding where student assessment
is primarily based on class assignments, projects,
midterms and final exams. Students live, work and socialize
on a small self-contained campus representing a tightlyknit
community. The pace of the 10 week Dartmouth term is
fast in comparison to a 15 week semester. The atmosphere
among the students on campus seems to visibly change from
a relaxed start of term, to an intense midterm and end of
term. Typically classes at Dartmouth are small (e.g., 25-50
students), but introductory classes are larger (e.g., 100-170),
making it difficult for a faculty to follow the engagement or
performance of students on an individual level. Unless students
contact a student dean or faculty about problems in their
lives, the impact of such challenges on performance remains
hidden.
To shine a light on student life we develop the StudentLife
smartphone app and sensing system to automatically infer human
behavior in an energy-efficient manner. The StudentLife
app integrates MobileEMA, a flexible ecological momentary
assessment [37] (EMA) component to probe students’ states
(e.g., stress, mood) across the term. We administer a number
of well-known pre-post health and behavioral surveys at the
start and end of term. We present the results from a deployment
of StudentLife on Google Nexus 4 Android phones at
Dartmouth College.
StudentLife makes a number of contributions. First, to the
best of our knowledge we are the first to use automatic and
continuous smartphone sensing to assess mental health, academic
performance and behavioral trends of a student body.
Second, we identify strong correlation between automatic
sensing data and a broad set of well-known mental wellbeing
measures, specifically, PHQ-9 depression, perceived
stress (PSS), flourishing, and loneliness scales. Results indicate
that automatically sensed conversation, activity, mobility,
and sleep have significant correlations with mental wellbeing
outcomes. We also observe strong correlations between
academic performance and automatic sensing data and
mental well-being. We find usage patterns of an online ed-ucational tool (i.e., Piazza) correlates with academic performance.
Third, we observe trends in the sensing data, termed
the Dartmouth term lifecycle, where students start the term
with high positive affect and conversation levels, low stress,
and healthy sleep and daily activity patterns. As the term progresses
and the workload increases, stress appreciably rises
while activity, sleep, conversation, positive affect, visits to
the gym and class attendance drop.
RELATED WORK
There is a growing interest in using smartphone sensing [31,
9, 12, 11, 40, 8, 15] to infer human dynamics and behavioral
health [7, 34, 25, 21, 18, 23, 35, 28, 30]. The StudentLife
study is influenced by a number of important behavioral studies:
1) the friends-and-families study [7], which uses Funf [3]
to collect data from 130 adult members (i.e., post-docs, university
employees) of a young family community to study fitness
intervention and social incentives; and 2) the reality mining
project [20], which uses sensor data from mobile phones
to study human social behavior in a group of students at MIT.
The authors show that call records, cellular-tower IDs, and
Bluetooth proximity logs accurately detect social networks
and daily activity.
There is little work on correlations between continuous and
automatic sensing data from smartphones and mental health
outcomes such as PHQ-9. However, the authors in [34] use
wearable sensors (i.e., Intel’s mobile sensing platform) to
study the physical and mental well-being of a group of 8 seniors
living in a continuing care retirement community over
a single week. The retirement community study [34] is the
first to find correlations with depression and continuous sensing
measures from wearables. In [33], the authors monitor
bipolar disorder in patients using wearable sensors, but the
project does not enable continuous sensing data. In [22, 10],
the authors present an approach that collects self-assessment
and sensor data on a smartphone as a means to study patients’
mood. They find that self-reported activity, stress, sleep and
phone usage are strongly correlated with self-reported mood.
Health Buddy [24] presents patients with a series of preprogrammed
questions about symptoms of depression and
suicide, allowing mental health service providers to monitor
patients’ symptoms. No continuously sensing is used. Mobilyze
is an intervention system [13] that uses smartphones to
predict self-reported states (e.g., location, alone, mood) using
machine learners. Results indicate that Mobilyze can predict
categorical contextual states (e.g., location, with friends) with
good accuracy but predicting internal states such as mood
show poorer predictive power.
There is a considerable interest in studying the health and
performance of students. However, no study has used smartphone
sensing to study these issues. In [39], the authors study
the effect of behaviors (i.e., social support, sleep habits, working
hours) on grade points based on 200 randomly chosen
students living on the campus at a large private university.
However, this study uses retrospective survey data manually
entered by users to assess health and performance. Watanabe
[41, 42] uses a wearable sensor device to investigate the
correlation between face-to-face interaction between students
during break times and scholastic performance.
STUDY DESIGN
In this section, we discuss how participants were recruited
from the student body, and then describe our data collection
process. We also discuss compliance and data quality issues
in this longitudinal study.
Participants
All participants in the study were voluntarily recruited from
the CS65 Smartphone Programming class [1], a computer science
programing class at Dartmouth College offered to both
undergraduate and graduate students during Spring term in
2013. This study is approved by the Institutional Review
Board at Dartmouth College. 75 students enrolled in the class
and 60 participants joined the study. As the term progressed,
7 students dropped out of the study and 5 dropped the class.
We remove this data from the dataset analyzed in the Results
Section. Among the 48 students who complete the study, 30
are undergraduates and 18 graduate students. The class demographics
are as follows: 8 seniors, 14 juniors, 6 sophomores,
2 freshmen, 3 Ph.D students, 1 second-year Masters
student, and 13 first-year Masters students. In terms of gender,
10 participants are female and 38 are male. In terms of
race, 23 participants are Caucasians, 23 Asians and 2 AfricanAmericans.
48 participants finished the pre psychological
surveys and 41 participants finished all post psychological
surveys.
All students enrolled in the class were offered unlocked Android
Nexus 4s to complete assignments and class projects.
Many students in the study had their own iPhones or Android
phones. We denote the students who use their own Android
phones to run the StudentLife sensing system as primary
users and those who use the Nexus 4s as secondary
users. Secondary users have the burden of carrying both their
own phones and the Nexus 4s during the study. We discuss
compliance and data quality of users in the Compliance and
Data Quality Section.
Study Procedure
The StudentLife study consists of orientation, data collection
and exit stages. In addition, we deployed a number of
management scripts and incentive mechanisms to analyze and
boost compliance, respectively.
Entry and Exit. During the orientation stage, participants
sign the consent form to join the study. Each student is given
a one-on-one tutorial of the StudentLife system and study.
Prior to signing the consent form, we detail the type of data to
be collected by the phone. Students are trained to use the app.
Students do not need to interact with the background sensing
or upload functions. They are shown how to respond to the
MobileEMA system. A series of entry health and psychological
baseline surveys are administered using SurveyMonkey
as discussed in the Results Section and shown in Table 1. As
part of the entry survey students provide demographic and
information about their spring term classes. All surveys are
administered using SurveyMonkey [6]. These surveys are premeasures which cover various aspects of mental and physical
health. Outcomes from surveys (e.g., depression scale) are
used as ground truth in the analysis. During the exit stage, we
administered an exit survey, interview and the same set of behavioral
and health surveys given during the orientation stage
as post measures.
Data Collection. The data collection phase lasted for 10
weeks for the complete spring term. After the orientation session,
students carried the phones with them throughout the
day. Automatic sensing data is collected without any user
interaction and uploaded to the cloud when the phone is being
recharged and under WiFi. During the collection phase,
students were asked to respond to various EMA questions as
they use their phones. This in-situ probing of students at multiple
times during the day provides additional state information
such as stress, mood, happiness, current events, etc. The
EMA reports were provided by a medical doctor and a number
of psychologists on the research team. The number of
EMAs fired each day varied but on average 8 EMAs per day
were administered. For example, on days around assignment
deadlines, we scheduled multiple stress EMAs. We set up
EMA schedules on a week-by-week basis. On some days
we administer the same EMA (e.g., PAM and stress) multiple
times per day. On average, we administer 3-13 EMA questions
per day (e.g., stress). The specific EMAs are discussed
in the Dataset Section.
Data Collection Monitoring. StudentLife includes a number
of management scripts that automatically produce statistics
on compliance. Each time we notice students’ phones not
uploading daily data (e.g., students left phones in their dorms
during the day), or gaps in weekly data (e.g., phones powered
down at night), or no response to EMAs, we sent emails to
students to get them back on track.
Incentives. To promote compliance and data quality, we offer
a number of incentives across the term. First, all students
receive a StudentLife T-shirt. Students could win prizes during
the study. At the end of week 3, we gave away 5 Jawbone
UPs to the 5 top student collectors randomly selected from
the top 15 collectors. We repeated this at week 6. We defined
the top collectors as those providing the most automatic sensing
and EMA data during the specific period. At the end of
the study, we gave 10 Google Nexus 4 phones to 10 collectors
who were randomly selected from the top 30 collectors over
the complete study period.
Privacy considerations. Participants’ privacy is a major
concern of our study. In order to protect participants’ personal
information, we fully anonymize each participant’s identity
with a random user id and kept the user id map separate from
all other project data so that the data cannot be traced back to
individuals. Call logs and SMS logs are one-way hashed so
that no one can get phone numbers or messages from the data.
Participants’ data is uploaded using encrypted SSL connections
to ensure that their data cannot be intercepted by thirdparties.
Data is stored on secured servers. When people left
the study their data was removed.
Compliance and Data Quality
The StudentLife app does not provide students any feedback
by design. We do not want to influence student behavior by
feedback, rather, we aim to unobtrusively capture student life.
Longitudinal studies such as StudentLife suffer from a drop
in student engagement and data quality. While automatic sensor
data collection does not introduce any burden other than
carrying a phone, collecting EMA data can be a considerable
burden. Students typically are compliant in responding to survey
questions at the start of a study, but as the novelty effect
wears off, student compliance drops.
There is a 60/40 split of iPhone/Android users in the study
group. Of the 48 students who completed the study, 11 are
primary phone users and 37 secondary users. One concern is
that the burden of carrying two phones for 10 weeks would result
in poorer data quality from the secondary users compared
to the primary users. Figure 1(a) shows the average hours of
sensor data we have collected from each participant during
the term. As expected, we observe that primary users are better
data sources, but there is no significant difference. We can
clearly see the trend of data dropping off as the term winds
down. Achieving the best data quality requires 24 hours of
continuous sensing each day. This means that users carry
their phones and power their phones at night. If we detect that
a student leaves their phone at the dorm during the day, or it
is powered down, then we remove that data from the dataset.
The overall compliance of collecting automatic sensing data
from primary and secondary users over the term is 87% and
81%, respectively.
Figure 1(b) shows the average number of EMA responses per
day for primary and secondary users. The figure does not capture
compliance per se, but it shows that secondary users are
slightly more responsive to EMAs than primary users. On average
we receive 5.8 and 5.4 EMAs per day per student across
the whole term from secondary and primary users, respectively.
As the term progresses there is a drop in both administered
EMAs and responses. However, even at the end of term,
we still receive over 2 EMAs per day per student. Surprisingly,
secondary users (72%) have better EMA compliance
than primary users (65%). During the exit survey, students favored
short PAM-style EMAs (see Figure 3(a)), complained
about the longer EMAs, and discarded repetitive EMAs as the
novelty wore off. By design, there is no notification when an
EMA is fired. Participants need to actively check their phone
to answer scheduled EMA questions. The EMA compliance
data (see Figure 1(b)) shows that there are no significant differences
between primary and secondary phone users. It indicates
that secondary phone users also used the study phone
when they were taking the phone with them. Therefore, the
study phone can capture the participants’ daily behavior even
it was not their primary phone.
In summary, Figure 1 shows the cost of collecting continuous
and EMA data across a 10-week study. There is a small
difference between primary and secondary collectors for continuous
sensing and EMA data, but the compliance reported
above is promising and gives confidence in the analysis discussed
in the Results Section. 0
 4
 8
 12
 16
 20
 24
1 8 15 22 29 36 43 50 57 64
1 2 3 4 5 6 7 8 9 10
Hours of data collected
Day
Week
overall
primary users
secondary users
(a) Automatic sensing data quality over the term
 0
 3
 6
 9
 12
 15
1 8 15 22 29 36 43 50 57 64
1 2 3 4 5 6 7 8 9 10
Number of responses
Day
Week
overall
primary users
secondary users
(b) EMA data quality over the term
Figure 1. Compliance and quality of StudentLife
data collected across the term.
accelerometer
microphone
light Sensor
GPS/Bluetooth
activity
conversation
sleep
location/colocation
mental health
academic
performance
mobile EMA
automatic
continuous
sensing
self-reports
behavioral
classifiers
statistical
analysis
SurveyMonkey
social
exercise
behavior
outcomes
mood
stress EMAs
Android phone cloud
others
Dartmouth
term lifecycle
StudentLife
cloud
Figure 2. StudentLife app, sensing and analytics system architecture.
STUDENTLIFE APP AND SENSING SYSTEM
In what follows, we describe the design of the StudentLife
app and sensing system, as shown in Figure 2.
Automatic and Continuous Sensing
We build on our prior work on the BeWell App [27] to provide
a framework for automatic sensing in StudentLife. The StudentLife
app automatically infers activity (stationary, walking,
running, driving, cycling), sleep duration, and sociability
(i.e., the number of independent conservations and their
durations). The app also collects accelerometer, proximity,
audio, light sensor readings, location, colocation, and application
usage. The inferences and other sensor data are temporarily
stored on the phone and are efficiently uploaded to
the StudentLife cloud when users recharge their phones under
WiFi. In what follows, we discuss the physical activity, sociability/conversation
and sleep inferences computed on the
phone which represent important heath well-being indicators
[27].
Activity Detection. We use the physical activity classifier
from our prior work [27, 29] to infer stationary, walking, running,
driving and cycling based on features extracted from accelerometer
streams. The activity classifier extracts features
from the preprocessed accelerometer stream, then applies a
decision tree to infer the activity using the features. The activity
classifier achieves overall 94% of accuracy [29]. (Note,
we conducted our study before Google announced the availability
of an activity recognition service for Android phones).
We extend our prior work to compute a daily activity duration,
and indoor and outdoor mobility measures, discussed as
follows. The activity classifier generates an activity label every
2 seconds. We are only interested in determining whether
a participant is moving. For each 10-min period, we calculate
the ratio of non-stationary inferences. If the ratio is greater
than a threshold, we consider this period active, meaning that
the user is moving. We add up all the 10-min active periods
as the daily activity duration. Typically, students leave their
dorms in the morning to go to various buildings on campus
during the day. Students spend a considerable amount of time
in buildings (e.g., cafes, lecture rooms, gym). We consider the
overall mobility of a student consists of indoor and outdoor
mobility. We compute the outdoor mobility (aka traveled distance)
as the distance a student travels around campus during
the day using periodic GPS samples. Indoor mobility is computed
as the distance a student travels inside buildings during
the day using WiFi scan logs. Dartmouth College has WiFi
coverage across all campus buildings. As part of the study,
we collect the locations of all APs in the network, and the
Wi-Fi scan logs including all encountered BSSIDs, SSIDs,
and their signal strength values. We use the BSSIDs and signal
strength to determine if a student is in a specific building.
If so, we use the output of activity classifier’s walk inference
to compute the activity duration as a measure of indoor mobility.
Note, that Dartmouth’s network operations provided
access to a complete AP map of the campus wireless network
as part of the IRB.
Conversation Detection. StudentLife implements two classifiers
on the phone for audio and speech/conversation detection:
an audio classifier to infer human voice, and a conversation
classifier to detect conversation. We process audio on
the fly to extract and record features. We use the privacysensitive
audio and conversation classifiers developed in our
prior work [34, 27]. Note, the audio classification pipeline
never records conversation nor analyses content. We first segment
the audio stream into 15-ms frames. The audio classi-
fier then extracts audio features, and uses a two-state hidden
Markov model (HMM) to infer speech segments. Our classifier
does not implement speaker identification. It simply
infers that the user is “around conversation” using the output
of the audio classifier as an input to a conservation classifier.
The output of the classification pipeline captures the
number of independent conversations and their duration. We
consider the frequency and duration of conversations around
a participant as a measure of sociability. Because not all(a) PAM EMA (b) Stress EMA
Figure 3. MobileEMA: First the PAM popup fires followed by one of the
StudentLife EMAs – in this example the single item stress EMA.
conservations are social, such as lectures and x-hours (i.e.,
class meetings outside lectures), we extend our conservation
pipeline in the cloud to remove conversations associated with
lectures and x-hours. We use student location to determine
if they attend lectures and automatically remove the conservation
data correspondingly from the dataset discussed in the
Dataset Section. We also keep track of class attendance for
all students across all classes, as discussed in the Results Section.
Sleep Detection. We implement a sleep classifier based
on our previous work [14, 27]. The phone unobtrusively infers
sleep duration without any special interaction with the
phone (e.g., the user does not have to sleep with the device).
The StudentLife sleep classifier extracts four types
of features: light features, phone usage features including
the phone lock state, activity features (e.g., stationary), and
sound features from the microphone. Any of these features
alone is a weak classifier for sleep duration because of the
wide variety of phone usage patterns. Our sleep model combines
these features to form a more accurate sleep model and
predictor. Specifically, the sleep model assumes that sleep
duration (Sl) is a linear combination of these four factors:
Sl =
P4
i=1 αi
·Fi
, αi ≥ 0 where αi
is the weight of the corresponding
factor. We train the model using the method described
in [14] with an accuracy of +/- 32 mins to the ground
truth. We extend this method to identify the sleep onset time
by looking at when the user is sedentary in term of activity,
audio, and phone usage. We compare the inferred sleep onset
time from a group of 10 students who use the Jawbone UP
during the study to collect sleep data. Our method predicts
bedtime where 95% of the inferences have an accuracy of +/-
25 mins of the ground truth. The output of our extended sleep
classifier is the onset of sleep (i.e., bedtime), sleep duration
and wake up time.
MobileEMA
We use in-situ ecological momentary assessment (EMA) [37]
to capture additional human behavior beyond what the surveys
and automatic sensing provide. The user is prompted
by a short survey (e.g., the single item [38] stress survey as
shown in Figure 3(b)) scheduled at some point during their
day. We integrate an EMA component into the StudentLife
app based on extensions to Google PACO [4]. PACO is an
extensible framework for quantified self experiments based
on EMA. We extend PACO to incorporate:
• photographic affect meter (PAM) [32] to capture participant’s
instantaneous mood;
• pop-up EMAs to automatically present a short survey to the
user when they unlock or use the phone; and,
• EMA schedule and sync feature to automatically push a
new EMA schedule to all participants and synchronize the
new schedule with StudentLife cloud.
PACO is a self-contained and complex backend app and service.
We extend and remove features and integrate the EMA
component into the StudentLife app and cloud. We set up
EMA questions and schedules using the PACO server-side
code [4]. The cloud pushes new EMA questions to the
phones. The StudentLife app sets up an alarm for each EMA
in the list and fires it by pushing it to the users’ phone screen
as a pop-up. We implement PAM [32] on the Nexus 4 as
part of the EMA component. PAM presents the user with a
randomized grid of 16 pictures from a library of 48 photos.
The user selects the picture that best fits their mood. Figure
3(a) shows the PAM pop-up asking the user to select one
of the presented pictures. PAM measures affect using a simple
visual interface. PAM is well suited to mobile usage because
users can quickly click on a picture and move on. Each
picture represents a 1-16 score, mapping to the Positive and
Negative Affect Schedule (PANAS) [43]. PAM is strongly
correlated with PANAS (r = 0.71, p < 0.001) for positive
affect. StudentLife schedules multiple EMAs per day. We
took the novel approach of firing PAM before showing one of
the scheduled EMAs (e.g., stress survey). Figure 3(b) shows
an EMA test after the PAM pop-up. We are interested in how
students’ mood changes during the day. By always preceding
any EMA with PAM, we guarantee a large amount of affect
data during the term.
STUDENTLIFE DATASET
Using the StudentLife system described in StudentLife Sensing
System Section, we collect a dataset from all subjects including
automatic sensor data, behavioral interferences, and
self-reported EMA data. Our ground truth data includes behavioral
and mental health outcomes computed from survey
instruments detailed in Table 1, and academic performance
from spring term and cumulative GPA scores provided by the
registrar. We discuss three epochs that are evident in the StudentLife
dataset. We uses these epochs (i.e., day 9am–6pm,
evening 6pm–12am, night 12am–9am) as a means to analyze
some of the data, as discussed in the Results Section. The
StudentLife dataset is publicly available [5].
Automatic Sensing Data. We collect a total of 52.6 GB of
sensing inference data from smartphones over 10 weeks. The
data consist of: 1) activity data, including activity duration
(total time duration the user moves per day), indoor mobilityFri
Thu
Wed
Tue
Mon
8 9 10 11 12 1 2 3 4 5 6 7 8
time of the day (8 am - 8 pm)
 0
 5
 10
 15
 20
 25
 30
 35
 40
 45
 50
number of participants
(a) Meeting time for all classes over the term
 0
 50
 100
 150
 200
 250
9 10 11 12 1 2 3 4 5 6 7
Number of instances
time of the day (9pm - 7am)
(b) Sleep onset time distribution for all students over the term
Figure 4. Statistics on class meeting times and sleep onset time (i.e., bedtime).
Table 1. Mental well-being surveys.
survey measure
patient health
questionnaire depression level
(PHQ-9) [26]
perceived stress scale stress level (PSS)[17]
flourishing scale flourishing level [19]
UCLA loneliness loneliness level scale [36]
Table 2. PHQ-9 depression scale interpretation and pre-post class outcomes.
depression minimal minor moderate moderately
severe severity severe
score 1-4 5-9 10-14 15-19 20-27
number of
students 17 15 6 1 1
(pre-survey)
number of
students 19 12 3 2 2
(post-survey)
and the total traveled distance (i.e., outdoor mobility) per day;
2) conversation data, including conversation duration and frequency
per day; 3) sleep data, including sleep duration, sleep
onset and waking time; and finally 4) location data, including
GPS, inferred buildings when the participant is indoors, and
the number of co-located Bluetooth devices.
Epochs. Students engage in different activities during the
day and night. As one would expect, sleep and taking classes
dominate a student’s week. Figure 4(a) shows the collective
timetable of class meetings for all the classes taken by the
students in the study. The darker the slot, the greater proportion
of students taking classes in the slot. We can observe
that Monday, Wednesday, Friday slots from 10:00-11:05 am
and the x-period on Thursday 12:00-12:50 pm are dominant
across the week; this is the teaching time for the CS65 Smartphone
Programming class which all students in the study are
enrolled in. Figure 4(a) clearly indicates that the timetable of
all classes ranges from 9am to 6pm – we label this as the day
epoch. Students are not taking classes for the complete period.
Many class, social, sports, and other activities take place
during the day epoch but class is dominant. The next dominant
activity is sleep. Students go to bed at different times.
Figure 4(b) shows the distribution of bedtime for all students
across the term. We see that most students go to bed between
12am and 4am but the switch from evening to night starts at
12am, as shown in Figure 4(b). We label the period between
12am and 9am as the night epoch, when most students are
working, socializing or sleeping – but sleep is the dominant
activity. We consider the remaining period between the end of
classes (6pm) and sleep (12am) as the evening epoch. We hypothesize
that this is the main study and socialization period
during weekdays. We define these three epochs as a means to
analyze data, as discussed in the Results Section. We appreciate
that weekdays are different from weekends but consider
epochs uniformly across the complete week. We also look
for correlations in complete days (e.g., Monday) and across
epochs (i.e., Monday day, evening and night).
EMA Data. Students respond to psychological and behavioral
EMAs on their smartphones that are scheduled, managed,
and synchronized using the MobileEMA component
integrated into StudentLife app. We collect a total of 35,295
EMA and PAM responses from 48 students over 10 weeks.
EMA and PAM data are automatically uploaded to the cloud
when students recharge their phones under WiFi. Students
respond to a number of scheduled EMAs including stress
(stress EMA), mood (mood EMA), sleep duration (sleep
EMA)(which we use to confirm the performance of our sleep
classifier), the number of people students encountered per day
(social EMA), physical exercise (exercise EMA), time spent
on different activities (activity EMA), and short personality
item (behavior EMA). All EMAs were either existing validated
EMAs (e.g., single item stress measure [38]) found in
the literature, or provided by psychologist on the team (e.g.,
mood EMA).
Survey Instrument Data. Table 1 shows the set of surveys
for measuring behavioral and mental well-being we administer
as part of our pre-post stages, as discussed in Study Design
Section. These questionnaires provide an assessment of
students’ depression, perceived stress, flourishing (i.e., selfperceived
success) and loneliness. Students complete surveys
using SurveyMonkey [6] one day prior to study commencement,
and complete them again one day after the study. Surveys
are administered on the phone and stored in the StudentLife
cloud (Figure 2). In what follows, we overview each
instrument. The Patient Health Questionnaire (PHQ-9) [26]
is a depression module that scores each of the 9 DSM-IV
criteria as 0 (not at all) to 3 (nearly every day). It is validated
for use in primary care. Table 2 shows the interpretation
of the scale and the number of students that fall into
each category for pre-post assessment. The perceived stress
scale (PSS) [17] measures the degree to which situations in
a person’s life are stressful. Psychological stress is the extent
to which a person perceives the demands on them exceed
their ability to cope [17]. Perceived stress is scored between
0 (least stressed) to 40 (most stressed). The perceived stress
scale can only be used for comparisons within a sample – in
our case 48 students. The flourishing scale [19] is an 8-item
summary measure of a person’s self-perceived success in important
areas such as relationships, self-esteem, purpose, and
optimism. The scale provides a single psychological wellbeing
score. Flourishing is scored between 8 (lowest) to 56
(highest). A high score represents a person with many psychological
resources and strengths. The final survey we administer
is the UCLA loneliness (version 3) [36] scale, whichTable 3. Correlations between automatic sensor data and PHQ-9 depression
scale.
automatic sensing data r p-value
sleep duration (pre) -0.360 0.025
sleep duration (post) -0.382 0.020
conversation frequency during day (pre) -0.403 0.010
conversation frequency during day (post) -0.387 0.016
conversation frequency during evening (post) -0.345 0.034
conversation duration during day (post) -0.328 0.044
number of co-locations (post) -0.362 0.025
is scored between 20 (least lonely) to 80 (most lonely). The
loneliness scale is a 20-item scale designed to measure a person’s
subjective feelings of loneliness as well as feelings of
social isolation. Low scores are considered a normal experience
of loneliness. Higher scores indicate a person is experiencing
severe loneliness. Table 4 shows the pre-post measures
(i.e., mean and standard deviation) for each scored survey
for all students. We discuss these assessments in the Results
Section.
Academic Data. We have access to transcripts from the registrar’s
office for all participants as a means to evaluate their
academic performance. We use spring and cumulative GPA
scores as ground truth outcomes. Undergraduates can receive
an A–E grade or I (incomplete). Students who get an Incomplete
must agree to complete the course by a specific date.
GPA ranges from 0 to 4. For the CS65 smartphone programming
class we had all the assignment and project deadlines –
no midterms or finals are given in this class. Students provide
deadlines of their other classes at the exit interview from their
calendars or returned assignments or exams.
RESULTS
In what follows, we discuss the main results from the StudentLife
study. We identify a number of significant correlations
between objective sensor data from smartphones and
mental well-being and academic performance outcomes. We
also identify a Dartmouth term lifecycle that captures the impact
of the term on behavioral measures representing an aggregate
term signature experienced by all students.
Correlation with Mental Health
We first consider correlations between automatic and objective
sensing data from smartphones and mental well-being.
We also discuss results from correlations between EMA data.
Specifically, we report on a number of significant correlations
between sensor and EMA data and pre-post survey
ground truth outcomes for depression (PHQ-9), flourishing,
perceived stress, and loneliness scales, as discussed in the
Dataset Section and shown in Table 4. We calculate the degree
of correlation between sensing/EMA data and outcomes
using the Pearson correlation [16] where r (−1 ≤ r ≤ 1)
indicates the strength and direction of the correlation, and p
the significance of the finding.
PHQ-9 Depression Scale. Table 2 shows the pre-post
PHQ-9 depression severity for the group of students in the
study. The majority of students experience minimal or minor
depression for pre-post measures. However, 6 students experience
moderate depression and 2 students are moderately
Table 4. Statistics of mental well-being surveys.
survey pre-study post-study
outcomes participants mean std participants mean std
depression 40 5.8 4.9 38 6.3 5.8
flourishing 40 42.6 7.9 37 42.8 8.9
stress 41 18.4 6.8 39 18.9 7.1
loneliness 40 40.5 10.9 37 40.9 10.5
Table 5. Correlations between automatic sensor data and flourishing
scale.
automatic sensing data r p-value
conversation duration (pre) 0.294 0.066
conversation duration during evening (pre) 0.362 0.022
number of co-locations (post) 0.324 0.050
severe or severely depressed at the start of term. At the end of
term more students (4) experience either moderately severe
or severely depressed symptoms. We find a number of significant
correlations (p ≤ 0.05) between sleep duration, conversation
frequency and duration, colocation (i.e., number of
Bluetooth encounters) and PHQ-9 depression, as shown Table
3. An inability to sleep is one of the key signs of clinical
depression [2]. We find a significant negative correlation between
sleep duration and pre (r = −0.360, p = 0.025) and
post (r = −0.382, p = 0.020) depression; that is, students
that sleep less are more likely to be depressed. There is a
known link between lack of sleep and depression. One of
the common signs of depression is insomnia or an inability to
sleep [2]. Our findings are inline with these studies on depression
[2]. However, we are the first to use automatic sensor
data from smartphones to confirm these findings. We also
find a significant negative association between conversation
frequency during the day epoch and pre (r = −0.403, p =
0.010) and post (r = −0.387, p = 0.016) depression. This
also holds for the evening epoch where we find a strong relationship
(r = −0.345, p = 0.034) between conversation
frequency and depression score. These results indicate that
students that have fewer conversational interactions are more
likely to be depressed. For conversation duration, we find a
negative association (r = −0.328, p = 0.044) during the day
epoch with depression. This suggests students who interact
less during the day period when they are typically social and
studying are more likely to experience depressive symptoms.
In addition, students that have fewer co-locations with other
people are more likely (r = −0.362, p = 0.025) to have a
higher PHQ-9 score. Finally, we find a significant positive
correlation (r = 0.412, p = 0.010) between the validated
single item stress EMA [38] and the post PHQ-9 scale. This
indicates that people that are stressed are also more likely to
experience depressive symptoms, as shown in Table 8.
Flourishing Scale. There are no literal interpretation of
flourishing scale, perceived stress scale (PSS) and UCLA
loneliness scale instruments, as discussed in the Dataset Section.
Simply put, however, the higher the score the more
flourishing, stressed and lonely a person is. We find a small
set of correlations (see Table 5) between sensor data and
flourishing. Conversation duration has a weak positive association
(r = 0.294, p = 0.066) during the 24 hour day
with flourishing. With regard to conversation during the
evening epoch we find a significant positive association (r =
0.362, p = 0.022) with flourishing. We also find that studentsTable 6. Correlations between automatic sensor data and perceived
stress scale (PSS).
automatic sensing data r p-value
conversation duration (post) -0.357 0.026
conversation frequency (post) -0.394 0.013
conversation duration during day (post) -0.401 0.011
conversation frequency during day (pre) -0.524 0.001
conversation frequency during evening (pre) -0.386 0.015
sleep duration (pre) -0.355 0.024
Table 7. Correlations between automatic sensor data and loneliness
scale.
automatic sensing data r p-value
activity duration (post) -0.388 0.018
activity duration for day (post) -0.326 0.049
activity duration for evening (post) -0.464 0.004
traveled distance (post) -0.338 0.044
traveled distance for day (post) -0.336 0.042
indoor mobility for day (post) -0.332 0.045
with more co-locations (r = 0.324, p = 0.050) are more
flourishing. These results suggest that students that are more
social and around people are more flourishing. Finally, positive
affect computed from the PAM self-report has significant
positive correlation (r = 0.470, p = 0.002) with flourishing,
as shown in Table 8. This is as we would imagine. People
who have good positive affect flourish.
Perceived Stress Scale. Table 6 shows the correlations
between sensor data and perceived stress scale (PSS). Conversation
frequency (r = −0.394, p = 0.013) and duration
(r = −0.357, p = 0.026) show significantly negative correlation
with post perceived stress. In addiction, we see more significant
negative associations if we just look at the day epoch.
Here, conversation frequency (r = −0.524, p = 0.001) and
duration (r = −0.401, p = 0.011) exhibit significant and
strong negative correlations with pre and post measure of
perceived stress, respectively. This suggests students in the
proximity of more frequent and longer conversations during
the day epoch are less likely to feel stressed. We cannot
distinguish between social and work study conversation,
however. We hypothesize that students work collaborative in
study groups. And these students make more progress and
are less stressed. There is also strong evidence that students
that are around more conversations in the evening epoch are
less stressed too. Specifically, there is strong negative relationship
(r = −0.386, p = 0.015) between conversation frequency
in the evening epoch and stress. There is also a link
between sleep duration and stress. Our results show that there
is a strong negative association (r = −0.355, p = 0.024) between
sleep duration and perceived stress. Students that are
getting more sleep experience less stress. Finally, we find significant
positive (r = 0.458, p = 0.003) and negative correlations
(r = −0.387, p = 0.012) between self-reported stress
levels and positive affect (i.e., PAM), respectively, and the
perceived stress scale. There is a strong connection between
daily reports of stress over the term and the pre-post perceived
stress scale, as shown in Table 8. Similarly, students that report
higher positive affect tend to be less stressed.
Loneliness Scale. We find a number of links between activity
duration, distance travelled, indoor mobility and the
Table 8. Correlations between EMA data and mental well-being outcomes.
mental health outcomes EMA r p-value
flourishing scale (pre) positive affect 0.470 0.002
loneliness (post) positive affect -0.390 0.020
loneliness (post) stress 0.344 0.037
PHQ-9 (post) stress 0.412 0.010
perceived stress scale (pre) positive affect -0.387 0.012
perceived stress scale (post) positive affect -0.373 0.019
perceived stress scale (pre) stress 0.458 0.003
perceived stress scale (post) stress 0.412 0.009
Table 9. Correlations between automatic sensing data and academic performance.
academic Sensing Data r p-value performance
spring GPA conversation duration (day) 0.356 0.033
spring GPA conversation frequency (day) 0.334 0.046
spring GPA indoor mobility -0.361 0.031
spring GPA indoor mobility during (day) -0.352 0.036
spring GPA indoor mobility during (night) -0.359 0.032
overall GPA activity duration -0.360 0.030
overall GPA activity duration std deviation -0.479 0.004
overall GPA indoor mobility -0.413 0.014
overall GPA indoor mobility during (day) -0.376 0.026
overall GPA indoor mobility during (night) -0.508 0.002
overall GPA number of co-locations 0.447 0.013
loneliness scale, as shown in Table 7. All our results relate
to correlations with post measures. Activity duration
during a 24 hour day has a significant negative association
(r = −0.388, p = 0.018) with loneliness. We can look at
the day and evening epochs and find correlations. There is a
negative correlation (r = −0.464, p = 0.004) between activity
duration in the evening epoch and loneliness. Distance
traveled during the complete day (r = −0.338, p = 0.044)
and the day epoch (r = −0.336, p = 0.042) show trends
with being lonely. Indoor mobility during the day epoch has
strong negative links (r = −0.332, p = 0.045) to loneliness.
Indoor mobility is a measure of how much a student is moving
in buildings during the day epoch. Students that are less
active and therefore less mobile are more likely to be lonely.
It is difficult to speculate about cause and effect. Maybe these
students move around less are more isolated (e.g., stay in their
dorm) because they have less opportunity to meet other students
outside of class. These students could feel lonely and
therefore more resigned not to seek out the company of others.
There is also no evidence that people who interact with
others regularly do not experience loneliness. This supports
our lack of findings between conversation and loneliness. The
PAM EMA data (positive affect) has a strong negative association
(r=−0.390, p = 0.020) with positive affect. In addition,
stress self-reports positively correlate (r = 0.344, p = 0.037)
with loneliness. Students who report higher positive affect
and less stress tend to report less loneliness, as shown in Table
8.
Correlation with Academic Performance
We examine correlations between sensing and EMA data and
academic performance. We also discuss how the use of an online
educational tool (i.e., Piazza) correlates with educational
performance. Piazza is a popular tool for students and instructors.
It offers a question and answer environment along
with key features for effective student collaboration. We usedPiazza in the smartphone programming class. We use student’s
cumulative GPA and spring GPA scores as the measure
of academic performance. The mean and standard deviations
for overall (i.e., cumulative) and spring GPA are (3.5, 0.38)
and (3.2, 1.0), respectively.
Table 9 shows a set of correlations between automatic sensor
data and academic performance. We find conversation
and indoor mobility of students have strong relationship
with academic performance for spring GPA. More specifi-
cally, we find conversation duration (r=0.356, p = 0.033)
and frequency (r=0.334, p = 0.046) during the day epoch
show a positive correlation with spring GPA. Spring GPA
is negatively related to indoor mobility across the complete
day (r = −0.361, p = 0.031), and during the day (r =
−0.352, p = 0.036) and night (r = −0.359, p = 0.032)
epochs. We also find links between these measures and cumulative
GPA, implying, sensor data collected during the spring
term is associated with overall college performance. Specifically,
activity duration (r = −0.360, p = 0.030) and its
standard deviation (r = −0.479, p = 0.004) is negatively
associated with cumulative GPA performance. Cumulative
GPA is negatively related indoor mobility across the complete
day (r = −0.413, p = 0.014), and during the day
(r = −0.376, p = 0.026) and night (r = −0.508, p = 0.002)
epochs. Interestingly, cumulative and spring GPA have similar
relationships with the same measures. However, the correlations
are more significant for cumulative GPA. There is a
significant positive connection (r = 0.447, p = 0.013) between
co-location and cumulative GPA; however this is not
found in spring GPA data. There is no prior work to the best
of our knowledge on studying the relationship between objective
sensor data and academic performance. Our results
indicate that students who are around more conservation do
better academically. These conversations could be social or
study based – it is not clear. The indoor mobility results indicate
that students who move around less while in campus
buildings (e.g., library, cafes, dorms) do better educationally.
Finally, we present analytics from using the Piazza site for
the CS65 Smartphone Programming class. All students used
Piazza to a great or lesser degree during the term. Piazza
acts as a bulletin board for class announcements but importantly
it provides a forum for students to help each other
collaboratively solve programming problems; that is, students
can post questions and others can respond. Piazza was
used heavily during the class. On average, students spent
43.21 days online, viewed 213.4 posts, posted 10.1 posts, and
asked/answered 3.57/1 questions. We find intuitive associations
between student usage patterns and their final grade
in the smartphone programming class. We analyzed usage
data for the 48 students in the study. We find that students
who are actively using the Piazza to read, post and respond
to questions do better academically in the smartphone programming
class. More specifically, the number of posts has
a strong positive correlation with their grade in the class
(r = 0.32, p = 0.039).
Dartmouth Term Lifecycle
We analyze the Dartmouth term lifecycle using both sensing
data and self-reported EMA data. Figure 5(a-c) shows key
behavioral measures and activities over the complete term.
Figure 5(a) shows EMA data for stress and positive affect
(PA), and automatic sensing data for sleep duration. Figure
5(b) shows continuous sensing trends specifically activity
duration, and conversation duration and frequency. Finally,
Figure 5(c) shows location based data from GPS and WiFi,
specifically, attendance across all classes, the amount of time
students spent in their dorms or at home, and visits to the
gym. We hypothesize that these sensing, EMA and location
based curves collectively represent a “Dartmouth term lifecycle”.
Whether these trends could be observed across a different
set of students at Dartmouth or more interestingly at a
different institution is future work. In what follow we discuss
workload across the term, mental well-being using EMA data
(i.e., stress and positive affect) and automatic sensing data
measures.
Academic Workload. We use the number of assignment
deadlines as a measure of the academic workload of students.
We collect class deadlines during exit interviews and validate
them against students’ calendars and returned assignments
dates. Figure 5 shows the average number of deadlines for
all student across each week of the term. The number of
deadlines peaks during the mid-term period in weeks 4 and
5. Interestingly, many classes taken by the students do not
have assignment deadlines during week 8. Final projects and
assignments are due in the last week of term before finals, as
shown in Figure 5(a). As discussed before, all study participants
take the same CS65 Smartphone Programming class,
for which they share the same deadlines. Among all CS65’s
lab assignment, Lab 4 is considered to be the most challenging
programming assignment. In the last week of term the
students need to give final presentations and live demos of
group projects for the smartphone programming class. The
students are told that app developed for the demo day has to
work to be graded. The demo is worth 30% of their overall
grade.
Self Reported Stress and Mood. Figure 5(a) shows the
average daily stress level and positive affect over the term for
all subjects as polynomial curves. Students are more stressed
during the mid-term (days 22-36) and finals periods. The positive
affect results show a similar trend. Students start the
term with high positive affect, which then gradually drops as
the term progresses. During the last week of term, students
may be stressed because of finals and class projects, with positive
affect dropping to its lowest point in the term. Overall,
the results indicate that the 10-week term is stressful for students
as workload increases. Figure 5(a) clearly shows that
students return to Dartmouth after spring break feeling the
most positive about themselves, the least stressed, the most
social in terms of conversation duration and the most active
(as shown in Figure 5(b)). As the term progresses toward
mid-term week, positive affect and activity duration plunge
and remain at low levels until the final weeks where positive
affect drops to its lowest point.
Automatic Sensing Data. We also study behavioral patterns
over the term by analyzing automatic sensing data. We0.1
0.12
0.14
0.16
1 8 15 22 29 36 43 50 57 64
1 2 3 4 5 6 7 8 9 10
Normalized ratio
Day
Week
midterm
deadlines
positive affect
sleep duration
stress
(a) EMA and sleep data
0.1
0.12
0.14
0.16
1 8 15 22 29 36 43 50 57 64
1 2 3 4 5 6 7 8 9 10
Normalized ratio
Day
Week
midterm
deadlines
activity duration
conv. duration
conv. freq.
(b) Automatic sensing data
0.1
0.12
0.14
0.16
1 8 15 22 29 36 43 50 57 64
1 2 3 4 5 6 7 8 9 10
Normalized ratio
Day
Week
midterm
deadlines
in-dorm duration
class attendance
gym visits
(c) Location-based data
Figure 5. Dartmouth term lifecycle: collective behavioral trends for all students over the term.
plot the polynomial fitting curves for sleep duration, activity
duration, conversation duration, conversation frequency,
as shown Figure 5(b), and location visiting patterns in Figure
5(c). Our key findings are as follows. We observe from
Figure 5(a) that sleep peaks at the end of the first week and
then drops off and is at its lowest during the mid-term exam
weeks. Sleep then improves until the last week of term when
it plummets to its lowest point in the cycle. As shown in
Figure 5(b) students start the term with larger activity duration,
which gradually drops as they become busier with
course work and other term activities. Finally, the activity
duration increases a little toward the end of term. Activity
duration reaches its lowest point on day 36 when students are
focused on completing the Lab 4 assignment – considered the
most demanding assignment in the smartphone programming
class.
The student’s level of face-to-face sociability starts high at the
start of term, then we observe an interesting conservation pattern,
as shown in Figure 5(b). As the term intensifies, conversation
duration drops almost linearly until week 8, and then
rebounds to its highest point at the end of term. Conversely,
the frequency of conservation increases from the start of term
until the start of midterms, and then it drops and recovers toward
the end of term. We speculate that sociability changes
from long social/study related interactions at the start of term
to more business-like interactions during midterms when students
have shorter conservations. At the end of term, students
are having more frequent, longer conversations.
Figure 5(c) provides a number of interesting insights based
on location based data. As the term progresses and deadlines
mount the time students spend at the significant places
in their lives radically changes. Visits to the gym plummet
during midterm and never rebound. The time students spend
in their dorm is low at the start of term perhaps due to socializing
then remains stable but drops during midterm. At week
8 time spent in dorms drops off and remains low until the end
of term. The most interesting curve is class attendance. We
use location data to determine if students attend classes. We
consider 100% attendance when all students attend all classes
and x-hours (if they exist). The term starts with 75% attendances
and starts dropping at week 3. It steadily declines to a
point at the end of term were only 25% of the class are attending
all their classes. Interestingly, we find no correlation between
class attendance and academic performance. We speculate
that students increasingly start missing classes as the
term progresses and the work load rises. However, absence
does not positively or negatively impact their grades. We put
this down to their self learning ability but plan to study this
further as part of future work.
It is difficult in this study to be concrete about the cause and
effect of this lifecycle. For example, stress or positive affect
could have nothing to do with workload and everything
to do with hardship of some sort (e.g., campus adjustment,
roommate conflicts, health issues). We speculate the intensive
workload compressed into a 10 week term puts considerable
demands on students. Those that excel academically
develop skills to effectively manage workload, social life and
stress levels.
CONCLUSION
In this paper, we presented the StudentLife sensing system
and results from a 10-week deployment. We discuss a number
of insights into behavioral trends, and importantly, correlations
between objective sensor data from smartphones and
mental well-being and academic performance for a set of
students at Dartmouth College. To the best of our knowledge,
this is the first-of-its-kind smartphone sensing system
and study. A natural question arises: Could we find similar
trends and correlations in a different student body? We are
currently working on a StudentLife study at the University of
Texas Austin for a class with a large number of remote students.
University of Texas Austin has semesters rather than
short terms. We are also planning a future study at Northeastern
University. Providing feedback of hidden states to
students and other stakeholders might be beneficial, but there
are many privacy issues to resolve. Students, deans, and clinicians
on campus all care about the health and well-being of
the student body. In this study, the professor running the study
had access to survey outcomes, sensing data, and EMAs for
students. In two cases, the professor intervened and did not
give failing grades to students who failed to complete a number
of assignments and missed lectures for several weeks.
Rather, they were given incomplete grades and completed assignments
over the summer. However, in other classes these
students took, their professors did not have such data available
and these students received failing grades. While access
to such data is under IRB and cannot be shared, the data
and intervention in grading enabled those students to return
to campus the following fall. If they had received 3 failing
grades, they would have been suspended for one term.REFERENCES